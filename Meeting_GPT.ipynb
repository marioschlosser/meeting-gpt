{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPWGFRBB3wdbawsidYPZZL+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marioschlosser/meeting-gpt/blob/main/Meeting_GPT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Introduction\n",
        "Upload a .wav file to your Google Drive that is a recording of a meeting. The Colab notebook will transcribe it using OpenAI Whisper, identify all speakers using pyannote.audio, and save two csv files: transcript.csv which has all speakers and what they're saying in sequential time, and speakers.csv which has the statistics of who speaks how much (and how fast). You need to give it a huggingface token (get your own on huggingface.co). Add the question you want answered about the meeting and copy and paste transcript.csv into ChatGPT."
      ],
      "metadata": {
        "id": "wecI4CyLgr0u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Give access to your Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "vZ1YIqClhFOM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55aa609f-ad83-4fbe-806e-6a5ea062b960"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Initialize huggingface.co\n",
        "huggingface_token = \"\" #@param {type:\"string\"}"
      ],
      "metadata": {
        "id": "fbVNlPajKips"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V9v23_IbdgTH"
      },
      "outputs": [],
      "source": [
        "#@title Load models and libraries\n",
        "\n",
        "%%capture\n",
        "!pip install torch torchvision torchaudio\n",
        "!pip install pyannote.audio\n",
        "!pip install openai-whisper\n",
        "\n",
        "from pyannote.audio import Pipeline\n",
        "import whisper\n",
        "import ssl\n",
        "import pandas as pd\n",
        "\n",
        "ssl._create_default_https_context = ssl._create_unverified_context\n",
        "\n",
        "pipeline = Pipeline.from_pretrained(\"pyannote/speaker-diarization\", use_auth_token=huggingface_token)\n",
        "\n",
        "# load Whisper model\n",
        "model = whisper.load_model(\"small.en\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Provide the Google Drive path to your input audio (.wav) file\n",
        "file_name = \"\" #@param {type:\"string\"}"
      ],
      "metadata": {
        "id": "zqcuhgIoEvKo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Run transcription and diarization and save to files\n",
        "diarization = pipeline(file_name)\n",
        "\n",
        "# copy diarization.itertracks into an array\n",
        "diarization_array = []\n",
        "for turn, _, speaker in diarization.itertracks(yield_label=True):\n",
        "    diarization_array.append([turn.start, turn.end, speaker])\n",
        "\n",
        "# compress successive rows with the same speaker into one row\n",
        "diarization_array_compressed = []\n",
        "for i in range(len(diarization_array)):\n",
        "    if i == 0:\n",
        "        diarization_array_compressed.append(diarization_array[i])\n",
        "    else:\n",
        "        if diarization_array[i][2] == diarization_array_compressed[-1][2]:\n",
        "            diarization_array_compressed[-1][1] = diarization_array[i][1]\n",
        "        else:\n",
        "            diarization_array_compressed.append(diarization_array[i])\n",
        "\n",
        "# print the compressed array\n",
        "for row in diarization_array_compressed:\n",
        "    print(row)\n",
        "  \n",
        "result = model.transcribe(file_name)\n",
        "segments = result[\"segments\"]\n",
        "\n",
        "# data has all the transcript data\n",
        "data = []\n",
        "for segment in segments:\n",
        "    meta = {\n",
        "        \"start\": segment[\"start\"],\n",
        "        \"end\": segment[\"end\"],\n",
        "        \"tokens\": segment[\"tokens\"],\n",
        "        \"text\": segment[\"text\"]\n",
        "    }\n",
        "    data.append(meta)\n",
        "\n",
        "# for each segment in data, match the closest start and end times to the diarization and get the speaker\n",
        "for segment in data:\n",
        "    start = segment[\"start\"]\n",
        "    end = segment[\"end\"]\n",
        "    # find the speaker in diarization array where the overlap is the largest\n",
        "    old_optimum = 0\n",
        "    for row in diarization_array_compressed:\n",
        "        optimum = min(end, row[1]) - max(start, row[0])\n",
        "        if optimum > old_optimum:\n",
        "            segment[\"speaker\"] = row[2]\n",
        "            old_optimum = optimum\n",
        "\n",
        "# create dataframe from data\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# save full transcript with speakers to csv\n",
        "df.groupby((df['speaker'] != df['speaker'].shift()).cumsum()).agg(text=('text',lambda x: \" \".join(x)), speaking=('speaker','first')).reset_index().dropna(subset=['speaking'])[['speaking', 'text']].apply(lambda x: ': '.join(x),axis=1).to_csv(path_or_buf=\"transcript.csv\", header=False, index=False)\n",
        "\n",
        "# calculate number of tokens per speaker as length of token list\n",
        "df[\"num_tokens\"] = df.tokens.apply(lambda x: len(x))\n",
        "\n",
        "# calculate number of tokens per speaker as sum over num_tokens\n",
        "tokens_per_speaker = df.groupby(\"speaker\")[\"num_tokens\"].sum()\n",
        "\n",
        "# first calculate speaking time as end - start, then sum over all segments per speaker\n",
        "df[\"speaking_time\"] = df.end - df.start\n",
        "speakingtime_per_speaker = df.groupby(\"speaker\")[\"speaking_time\"].sum()\n",
        "\n",
        "# join the two series on speaker\n",
        "speakers = tokens_per_speaker.to_frame().join(speakingtime_per_speaker.to_frame())\n",
        "\n",
        "# calculate tokens per minute\n",
        "speakers[\"tokens_per_minute\"] = speakers.num_tokens / speakers.speaking_time * 60\n",
        "\n",
        "# extract one sentence per speaker and start time\n",
        "speakers.index = speakers.index.astype(str)\n",
        "speakers[\"start\"] = df.groupby(\"speaker\")[\"start\"].min()\n",
        "speakers[\"text\"] = df.groupby(\"speaker\")[\"text\"].first()\n",
        "\n",
        "# show speaker, text and start time\n",
        "for speaker in speakers.index:\n",
        "    print(speaker, \"Start time: \", speakers.start[speaker], \"Tokens per minute: \", speakers.tokens_per_minute[speaker], \"Speaking time: \", speakers.speaking_time[speaker], speakers.text[speaker])\n",
        "\n",
        "# save to csv\n",
        "speakers.to_csv(\"speakers.csv\")"
      ],
      "metadata": {
        "id": "Nzr1p-dSvrBp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}