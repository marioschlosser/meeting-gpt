{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNO72I5Gx3jcB7F9XfwAUTx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marioschlosser/meeting-gpt/blob/main/Meeting_GPT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Give access to your Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "vZ1YIqClhFOM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55aa609f-ad83-4fbe-806e-6a5ea062b960"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Initialize huggingface.co\n",
        "huggingface_token = \"\" #@param {type:\"string\"}"
      ],
      "metadata": {
        "id": "fbVNlPajKips"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V9v23_IbdgTH"
      },
      "outputs": [],
      "source": [
        "#@title Load models and libraries\n",
        "\n",
        "%%capture\n",
        "!pip install torch torchvision torchaudio\n",
        "!pip install pyannote.audio\n",
        "!pip install openai-whisper\n",
        "\n",
        "from pyannote.audio import Pipeline\n",
        "import whisper\n",
        "import ssl\n",
        "import pandas as pd\n",
        "\n",
        "ssl._create_default_https_context = ssl._create_unverified_context\n",
        "\n",
        "pipeline = Pipeline.from_pretrained(\"pyannote/speaker-diarization\", use_auth_token=huggingface_token)\n",
        "\n",
        "# load Whisper model\n",
        "model = whisper.load_model(\"small.en\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Provide the Google Drive path to your input audio (.wav) file\n",
        "file_name = \"\" #@param {type:\"string\"}"
      ],
      "metadata": {
        "id": "zqcuhgIoEvKo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Run transcription and diarization and save to files\n",
        "diarization = pipeline(file_name)\n",
        "\n",
        "# copy diarization.itertracks into an array\n",
        "diarization_array = []\n",
        "for turn, _, speaker in diarization.itertracks(yield_label=True):\n",
        "    diarization_array.append([turn.start, turn.end, speaker])\n",
        "\n",
        "# compress successive rows with the same speaker into one row\n",
        "diarization_array_compressed = []\n",
        "for i in range(len(diarization_array)):\n",
        "    if i == 0:\n",
        "        diarization_array_compressed.append(diarization_array[i])\n",
        "    else:\n",
        "        if diarization_array[i][2] == diarization_array_compressed[-1][2]:\n",
        "            diarization_array_compressed[-1][1] = diarization_array[i][1]\n",
        "        else:\n",
        "            diarization_array_compressed.append(diarization_array[i])\n",
        "\n",
        "# print the compressed array\n",
        "for row in diarization_array_compressed:\n",
        "    print(row)\n",
        "  \n",
        "result = model.transcribe(file_name)\n",
        "segments = result[\"segments\"]\n",
        "\n",
        "# data has all the transcript data\n",
        "data = []\n",
        "for segment in segments:\n",
        "    meta = {\n",
        "        \"start\": segment[\"start\"],\n",
        "        \"end\": segment[\"end\"],\n",
        "        \"tokens\": segment[\"tokens\"],\n",
        "        \"text\": segment[\"text\"]\n",
        "    }\n",
        "    data.append(meta)\n",
        "\n",
        "# for each segment in data, match the closest start and end times to the diarization and get the speaker\n",
        "for segment in data:\n",
        "    start = segment[\"start\"]\n",
        "    end = segment[\"end\"]\n",
        "    # find the speaker in diarization array where the overlap is the largest\n",
        "    old_optimum = 0\n",
        "    for row in diarization_array_compressed:\n",
        "        optimum = min(end, row[1]) - max(start, row[0])\n",
        "        if optimum > old_optimum:\n",
        "            segment[\"speaker\"] = row[2]\n",
        "            old_optimum = optimum\n",
        "\n",
        "# create dataframe from data\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# save full transcript with speakers to csv\n",
        "df.groupby((df['speaker'] != df['speaker'].shift()).cumsum()).agg(text=('text',lambda x: \" \".join(x)), speaking=('speaker','first')).reset_index().dropna(subset=['speaking'])[['speaking', 'text']].apply(lambda x: ': '.join(x),axis=1).to_csv(path_or_buf=\"transcript.csv\", header=False, index=False)\n",
        "\n",
        "# calculate number of tokens per speaker as length of token list\n",
        "df[\"num_tokens\"] = df.tokens.apply(lambda x: len(x))\n",
        "\n",
        "# calculate number of tokens per speaker as sum over num_tokens\n",
        "tokens_per_speaker = df.groupby(\"speaker\")[\"num_tokens\"].sum()\n",
        "\n",
        "# first calculate speaking time as end - start, then sum over all segments per speaker\n",
        "df[\"speaking_time\"] = df.end - df.start\n",
        "speakingtime_per_speaker = df.groupby(\"speaker\")[\"speaking_time\"].sum()\n",
        "\n",
        "# join the two series on speaker\n",
        "speakers = tokens_per_speaker.to_frame().join(speakingtime_per_speaker.to_frame())\n",
        "\n",
        "# calculate tokens per minute\n",
        "speakers[\"tokens_per_minute\"] = speakers.num_tokens / speakers.speaking_time * 60\n",
        "\n",
        "# extract one sentence per speaker and start time\n",
        "speakers.index = speakers.index.astype(str)\n",
        "speakers[\"start\"] = df.groupby(\"speaker\")[\"start\"].min()\n",
        "speakers[\"text\"] = df.groupby(\"speaker\")[\"text\"].first()\n",
        "\n",
        "# show speaker, text and start time\n",
        "for speaker in speakers.index:\n",
        "    print(speaker, \"Start time: \", speakers.start[speaker], \"Tokens per minute: \", speakers.tokens_per_minute[speaker], \"Speaking time: \", speakers.speaking_time[speaker], speakers.text[speaker])\n",
        "\n",
        "# save to csv\n",
        "speakers.to_csv(\"speakers.csv\")"
      ],
      "metadata": {
        "id": "Nzr1p-dSvrBp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}